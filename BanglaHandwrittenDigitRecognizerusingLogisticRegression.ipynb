{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "160204072_Problem#1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_WNDMi_khaa"
      },
      "source": [
        "#Problem#1 : Bangla Hand Written Digit Recognizer\r\n",
        "Appling only **logistic** regression for the **NumtaDB** dataset and building a **multiclass** classification model that can recognize [0-9] Bengali handwritten digits with different hyperparameter settings.\r\n",
        "\r\n",
        "##Logistic Regression for multiclass classification:\r\n",
        "**Logistic** regression is a very popular machine learning technique. Logistic regression is used when the dependent variable is categorical.\r\n",
        "The **goal** of logistic regression is to minimize the error between its\r\n",
        "predictions and training data. \r\n",
        "if given x feature vector,\r\n",
        "\r\n",
        "$\\;\\;\\;Y=P(y=1|x), \\;where\\; 0<=Y<=1$\r\n",
        "\r\n",
        "Logistic regression uses a **sigmoid** function to predict the output. The sigmoid function returns a value from 0 to 1.\r\n",
        "\r\n",
        "$\\;\\;\\;s=\\sigma (w^Tx+b)=\\sigma(z)=\\frac{1}{1+e^{-z}}$\r\n",
        "\r\n",
        "The implementation of **Multiclass** classification follows the same ideas as the binary classification.In multi-class classification, we have more than two classes. \r\n",
        "\r\n",
        "For multiple classes, logistic regression uses a **softmax** function to predict the output. Applying the softmax function, it gives an array of result. These represent the probability for the data point belonging to each class. The sum of all the values in the result array is 1.\r\n",
        "\r\n",
        "\r\n",
        "$\\;\\;\\;a_i=\\frac{e^{z_i}}{\\sum^c_{k=1}   e^{-z_k}}$\r\n",
        "$\\;\\;\\;where \\sum^c_{i=1} a_i =1 $\r\n",
        "\r\n",
        "Here, we are implementing Bangla Handwritten Digit Recognizer using logistic regression.For this, we need image as input and we have 10 classes(0-9) as output. So, we are implementig multi-class classification.\r\n",
        "\r\n",
        "##Dataset: \r\n",
        "One of the key topics of NLP is optical character recognition (OCR). To build an OCR in Bengali language, digit classification provides a convenient starting point. **NumtaDB** s a large dataset (85,000+) of Bengali digits which can be used by researchers for benchmarking their algorithm.\r\n",
        "\r\n",
        "<div align=\"center\">\r\n",
        "<img src=\"https://drive.google.com/uc?id=1LvkNwV1My2RniR_JsbasBET1fa97eMQu\" width=\"500\">\r\n",
        "</div>\r\n",
        "\r\n",
        "- **input dimension:**\r\n",
        "  - Size of an image: $28 \\times 28 = 744$\r\n",
        "- **output dimension: 10**\r\n",
        "  - 0,1,2,3,4,5,6,7,8,9\r\n",
        "\r\n",
        "\r\n",
        "The steps to implement bangla hand written digit recognizer will be the following:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6OwH8tgYERm"
      },
      "source": [
        "##Step#1: Load the dataset\r\n",
        "The NumtaDb dataset is directly loaded from Kaggle. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MiWWWFdRMadK",
        "outputId": "232a0cbb-e063-461d-c2f7-cdc4af1af809"
      },
      "source": [
        "!pip install kaggle\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.6/dist-packages (1.5.10)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.0.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2020.12.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.41.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.8.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (2.10)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle) (1.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "FqgGjMywMgK6",
        "outputId": "f57ebb90-ecbb-47f5-924d-a0caa1b1d7e3"
      },
      "source": [
        "from google.colab import files\r\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-af9fa1d6-e849-4faa-b210-2d00785e9abd\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-af9fa1d6-e849-4faa-b210-2d00785e9abd\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"anikatanzimaboni\",\"key\":\"13fc7f14dc7fd8e9f087e64c6b2a14f2\"}'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvDmcKgwMkVI"
      },
      "source": [
        "!mkdir -p ~/.kaggle\r\n",
        "!cp kaggle.json ~/.kaggle/\r\n",
        "!chmod 600 ~/.kaggle/kaggle.json # changing permission"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BeGmafr3Mmqr",
        "outputId": "57bcc51b-add6-4038-c284-8c3a5f19590a"
      },
      "source": [
        "!kaggle datasets download -d BengaliAI/numta"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading numta.zip to /content\n",
            "100% 1.91G/1.91G [00:18<00:00, 37.4MB/s]\n",
            "100% 1.91G/1.91G [00:19<00:00, 107MB/s] \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0vO1MxUapn0"
      },
      "source": [
        "**numta.zip** file is downloaded from Kaggle directly. After unzipping it, we get the actual dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6DPCAzqMojT",
        "outputId": "596f7e8c-b8fa-4f08-8630-cf7f3b9cc7c8"
      },
      "source": [
        "from zipfile import ZipFile\r\n",
        "f_name= 'numta.zip'\r\n",
        "with ZipFile(f_name,'r') as zip:\r\n",
        "  zip.extractall()\r\n",
        "  print('Done')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FIC8KTGMpVQ"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import torchvision\r\n",
        "import torchvision.transforms as transforms\r\n",
        "import torchvision.datasets as dsets\r\n",
        "import os\r\n",
        "from os import path\r\n",
        "import shutil"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLMDFcFybjl5"
      },
      "source": [
        "In the dataset, the sources are labeled from 'a' to 'e'. The training sets have separate subsets depending on the source of the data (training-a, training-b, etc.) and the details are also in different csv files (training-a.csv, training-b.csv, etc.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xckL9lYrMsrL"
      },
      "source": [
        "def showRawTrainingSamples(csv_filename):\r\n",
        "  df = pd.read_csv(csv_filename)\r\n",
        "  print(csv_filename)\r\n",
        "  print(df.columns)\r\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKRcPqioMuv2",
        "outputId": "b8235b44-9fa7-40bc-82a1-f85c5891bb83"
      },
      "source": [
        "import pandas as pd\r\n",
        "a_csv = showRawTrainingSamples('training-a.csv')\r\n",
        "b_csv = showRawTrainingSamples('training-b.csv')\r\n",
        "c_csv = showRawTrainingSamples('training-c.csv')\r\n",
        "d_csv = showRawTrainingSamples('training-d.csv')\r\n",
        "e_csv = showRawTrainingSamples('training-e.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training-a.csv\n",
            "Index(['filename', 'original filename', 'scanid', 'digit',\n",
            "       'database name original', 'contributing team', 'database name'],\n",
            "      dtype='object')\n",
            "training-b.csv\n",
            "Index(['filename', 'original filename', 'scanid', 'digit',\n",
            "       'database name original', 'contributing team', 'database name'],\n",
            "      dtype='object')\n",
            "training-c.csv\n",
            "Index(['filename', 'original filename', 'scanid', 'digit',\n",
            "       'database name original', 'contributing team', 'database name'],\n",
            "      dtype='object')\n",
            "training-d.csv\n",
            "Index(['original filename', 'scanid', 'digit', 'num', 'database name original',\n",
            "       'database name', 'filename'],\n",
            "      dtype='object')\n",
            "training-e.csv\n",
            "Index(['filename', 'original filename', 'districtid', 'institutionid',\n",
            "       'gender', 'age', 'datestamp', 'scanid', 'digit',\n",
            "       'database name original', 'database name'],\n",
            "      dtype='object')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDgXoReRchq7"
      },
      "source": [
        "From the csv files, we only need the information of the file names and their labels which column names are '**filename**' and '**digit**' respectively. So we truncate the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOtcaSwPM2ia"
      },
      "source": [
        "def truncateColumns(csv_file):\r\n",
        "  csv_file = csv_file[['filename', 'digit']]\r\n",
        "  print('Done')\r\n",
        "  return csv_file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xK5-jdC6M9BH",
        "outputId": "e128ab4a-b31b-46fd-b734-8d2a7cb76395"
      },
      "source": [
        "a_csv = truncateColumns(a_csv)\r\n",
        "b_csv = truncateColumns(b_csv)\r\n",
        "c_csv = truncateColumns(c_csv)\r\n",
        "d_csv = truncateColumns(d_csv)\r\n",
        "e_csv = truncateColumns(e_csv)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done\n",
            "Done\n",
            "Done\n",
            "Done\n",
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RHyPFxBdF1s"
      },
      "source": [
        "All the csv files are merged into one for further process. So, in total, there are 72045 data for training and testing the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DlcSqyudNCbr",
        "outputId": "0a87bc39-509e-418b-e7b6-15b5170c815a"
      },
      "source": [
        "total_csv = [a_csv,b_csv,c_csv,d_csv,e_csv]\r\n",
        "merged_csv = pd.concat(total_csv)\r\n",
        "print(len(merged_csv))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "72045\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbdVEKc0dyle"
      },
      "source": [
        "Similarly, all  the images from different directory are merged into one directory named \"**trainAll2**\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYAcFesHNIot"
      },
      "source": [
        "TRAIN_PATH = 'trainALL2'\r\n",
        "os.mkdir(TRAIN_PATH) #creating the directory\r\n",
        "\r\n",
        "def processImages(folder_name):\r\n",
        "  src = folder_name + '/'\r\n",
        "  dir_folders = os.listdir(src)\r\n",
        "  for dir_name in dir_folders:\r\n",
        "    file_name = os.path.join(src, dir_name)\r\n",
        "    if os.path.isfile(file_name):\r\n",
        "      shutil.copy(file_name, TRAIN_PATH) #images are copied to trainAll2\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgCm3-QfNeGq",
        "outputId": "cded42ae-a5de-4251-a716-fcd807cc3320"
      },
      "source": [
        "\r\n",
        "processImages('training-a')\r\n",
        "print('A Done')\r\n",
        "processImages('training-c')\r\n",
        "print('C Done')\r\n",
        "processImages('training-d')\r\n",
        "print('D Done')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A Done\n",
            "C Done\n",
            "D Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNxm5eLMPXgS",
        "outputId": "4340cbac-2236-4322-9ff7-d8a2a5673e08"
      },
      "source": [
        "processImages('training-b')\r\n",
        "print('B Done')\r\n",
        "processImages('training-e')\r\n",
        "print('E Done')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "B Done\n",
            "E Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGbee_4be8i4"
      },
      "source": [
        "##Step#2: Prepare (augment) the dataset\r\n",
        "The loaded dataset is still non trival. PyTorch provides many tools to make data loading easy and hopefully, to make the code more readable. \r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IINdrHAbNjEh"
      },
      "source": [
        "import torch.optim as optim\r\n",
        "from torch.utils.data import Dataset\r\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\r\n",
        "from torchvision import datasets, transforms, models\r\n",
        "import matplotlib\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from PIL import Image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7MJADC8e7S4"
      },
      "source": [
        "###Custom Dataset Class\r\n",
        "\r\n",
        "**`torch.utils.data.Dataset`** is an abstract class representing a dataset. To prepare the dataset, we need a** custom Dataset Class**. Our custom dataset should inherit Dataset and override the following methods:\r\n",
        "\r\n",
        "- **`__len__`** so that len(dataset) returns the size of our dataset.\r\n",
        "\r\n",
        "- **`__getitem__`** to support the indexing such that dataset[i] can be used to get ith sample. \r\n",
        "We will read the csv in` __init__` but leave the reading of images to `__getitem__`.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7VVgtENNnm1"
      },
      "source": [
        "class Dataset(Dataset):\r\n",
        "    def __init__(self, df, root, transform=None):\r\n",
        "        self.data = df\r\n",
        "        self.root = root\r\n",
        "        self.transform = transform\r\n",
        "\r\n",
        "        \"\"\"\r\n",
        "            df : Path to the csv file with annotations.\r\n",
        "            root : Directory with all the images.\r\n",
        "            transform : Optional transform to be appliedon a sample.\r\n",
        "        \"\"\"\r\n",
        "        \r\n",
        "    def __len__(self):\r\n",
        "        return len(self.data)\r\n",
        "    \r\n",
        "    def __getitem__(self, index):\r\n",
        "        item = self.data.iloc[index]\r\n",
        "        \r\n",
        "        path = self.root + \"/\" + item[0]\r\n",
        "        image = Image.open(path).convert('L')\r\n",
        "        label = item[1]\r\n",
        "        \r\n",
        "        if self.transform is not None:\r\n",
        "            image = self.transform(image)\r\n",
        "            \r\n",
        "        return image, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfXjh9nBnICj"
      },
      "source": [
        "###Compose Transforms\r\n",
        "The samples are not of the same size. Our model expect the images of a fixed size. `torchvision.transforms.Compose` is a simple callable class which allows us to do this.\r\n",
        "\r\n",
        "- **Resize((28,28))** resize/crop the image to 28*28 dimension\r\n",
        "- **ToTensor()** takes a PIL image (or np.int8 NumPy array) with shape (n_rows, n_cols, n_channels) as input and returns a PyTorch tensor with floats between 0 and 1 and shape (n_channels, n_rows, n_cols).\r\n",
        "- **Normalize(mean,std)** subtracts the mean and divides by the standard deviation of the floating point values in the range [0, 1].\r\n",
        "- As mean =0.5 and std =0.5, an image is normalized to [-1,1] here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5S5Q-GVnNq70",
        "outputId": "4181ed33-380d-4dab-c4fb-7d4e485d62c4"
      },
      "source": [
        "mean = [0.5,]\r\n",
        "std = [0.5, ]\r\n",
        "\r\n",
        "train_transform = transforms.Compose([\r\n",
        "    transforms.Resize((28,28)),\r\n",
        "    transforms.ToTensor(),\r\n",
        "    transforms.Normalize(mean, std)\r\n",
        "])\r\n",
        "\r\n",
        "test_transform = transforms.Compose([\r\n",
        "        transforms.Resize((28,28)),\r\n",
        "        transforms.ToTensor(),\r\n",
        "        transforms.Normalize(mean, std)\r\n",
        "])\r\n",
        "\r\n",
        "'''Instantiate the Dataset Class'''\r\n",
        "train_dataset  = Dataset(merged_csv, TRAIN_PATH, train_transform)\r\n",
        "test_dataset = Dataset(merged_csv, TRAIN_PATH, test_transform)\r\n",
        "\r\n",
        "print(\"Trainig Samples: \",len(train_dataset))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trainig Samples:  72045\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jC6SZr8jLTl"
      },
      "source": [
        "##step#3 : Make Data Iterable\r\n",
        "- We split the training set to 90% and test set to 10%. That means a 90:10 ratio. To split the data we have used **`torch.utils.data.sampler`** class.\r\n",
        "- We will use the **DataLoader** class to make our dataset iterable. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dW5cPFvNsCv",
        "outputId": "7ac57092-2d5f-48bc-c2aa-7d08fcffff52"
      },
      "source": [
        "import numpy as np\r\n",
        "#batch size\r\n",
        "#batch_size = 100\r\n",
        "batch_size = 64\r\n",
        "#batch_size = 32 \r\n",
        "#batch_size=45\r\n",
        "# split data 10% for testing\r\n",
        "test_size = 0.1\r\n",
        "\r\n",
        "# obtain training indices that will be used for validation\r\n",
        "len_train = len(train_dataset)\r\n",
        "\r\n",
        "# mix data\r\n",
        "# index of num of train\r\n",
        "indices = list(range(len_train))\r\n",
        "# random the index\r\n",
        "np.random.shuffle(indices)\r\n",
        "split = int(np.floor(test_size * len_train))\r\n",
        "# divied into two part\r\n",
        "train_idx, test_idx = indices[split:], indices[:split]\r\n",
        "\r\n",
        "# define the sampler\r\n",
        "train_sampler = SubsetRandomSampler(train_idx)\r\n",
        "test_sampler = SubsetRandomSampler(test_idx)\r\n",
        "\r\n",
        "# prepare loaders\r\n",
        "train_loader = torch.utils.data.DataLoader(\r\n",
        "    train_dataset, batch_size=batch_size,\r\n",
        "    sampler=train_sampler) #No need to shuffle here, because sampler has shuffled data already\r\n",
        "\r\n",
        "test_loader = torch.utils.data.DataLoader(\r\n",
        "    test_dataset, batch_size=batch_size,\r\n",
        "    sampler=test_sampler)\r\n",
        "\r\n",
        "print(\"Train dataloader:{}\".format(len(train_loader)))\r\n",
        "print(\"Test dataloader:{}\".format(len(test_loader)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train dataloader:1014\n",
            "Test dataloader:113\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbL34xd2rcNU"
      },
      "source": [
        "Showing the information of one sample data and the images:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OG10yNrNzg-",
        "outputId": "01f12427-6ad2-44ce-f6cf-2f1b24dc9683"
      },
      "source": [
        "'''Informations of one sample data'''\r\n",
        "# One Image Size\r\n",
        "print(train_dataset[22][0].size())\r\n",
        "print(train_dataset[22][0].numpy().shape)\r\n",
        "# First Image Label\r\n",
        "print(train_dataset[22][1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 28, 28])\n",
            "(1, 28, 28)\n",
            "6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drxPZf7eN09N"
      },
      "source": [
        "import matplotlib.pyplot as plt \r\n",
        "import numpy as np\r\n",
        "\r\n",
        "print(\"Label:\")\r\n",
        "print(train_dataset[0][1])\r\n",
        "show_img = train_dataset[0][0].numpy().reshape(28, 28)\r\n",
        "plt.imshow(show_img, cmap='gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hG6pOOoCN8KV"
      },
      "source": [
        "print(\"Label:\")\r\n",
        "print(train_dataset[201][1])\r\n",
        "\r\n",
        "show_img = train_dataset[201][0].numpy().reshape(28, 28)\r\n",
        "plt.imshow(show_img, cmap='gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQ1mTWu8rxn2"
      },
      "source": [
        "##Step#4: Create the Model Class\r\n",
        "\r\n",
        "In order to set up the model class, we need to initialize the model type and declare the forward pass. \r\n",
        "\r\n",
        "We initialize our model with this linear layer: **`torch.nn.Linear(input_size, num_classes)`** which\r\n",
        "applies a linear transformation to the incoming data:  $y=W^T∗x+b$.\r\n",
        "\r\n",
        "Parameters:\r\n",
        "\r\n",
        "- input_size – size of each input sample (i.e. size of x)\r\n",
        "- num_classes – size of each output sample (i.e. size of y)\r\n",
        "- bias – If set to False, the layer will not learn an additive bias. Default: True\r\n",
        "\r\n",
        "Next is to define the forward pass function. The forward pass refers to the calculation process of the output data from the input. The function takes x as its input and outputs the logits. \r\n",
        "\r\n",
        "We can simply apply **`functional.softmax`** to our current linear output from the forward pass: **`probas = functional.softmax(logits,dim=1)`**. Probas will be the predicted Y.\r\n",
        "\r\n",
        "Softmax is used because it is a multi class classification for logistic regression.\r\n",
        "\r\n",
        "The complete model class is defined below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGQzrGu7OACr"
      },
      "source": [
        "class LogisticRegressionModel(nn.Module):\r\n",
        "    def __init__(self, input_size, num_classes):\r\n",
        "        super().__init__()\r\n",
        "        self.linear = nn.Linear(input_size, num_classes)\r\n",
        "        \"\"\"\r\n",
        "          In the forward function we accept a Variable of input data and we must return\r\n",
        "          a Variable of output data. We can use Modules defined in the constructor as\r\n",
        "          well as arbitrary operators on Variables.\r\n",
        "        \"\"\"\r\n",
        "    def forward(self, x):\r\n",
        "        logits  = self.linear(x)\r\n",
        "        probas = F.softmax(logits, dim=1)\r\n",
        "        return logits, probas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaNiXpgfsMXH"
      },
      "source": [
        "##Step#5:Instantiate the Model Class\r\n",
        "\r\n",
        "####**Modelparameter:**\r\n",
        "Model parameters are learned during training when we optimize a loss function using something like gradient descent.\r\n",
        "\r\n",
        "####**Hyperparameter:**\r\n",
        "Parameters which define the model architecture are referred to as hyperparameters. Hyperparameters are not model parameters and they cannot be directly trained from the data. \r\n",
        "These values are fixed before the training of the data begins. They deal with parameters such as learning_rate,num_iters etc.\r\n",
        "\r\n",
        "- **learning_rate**: How quickly the model should be able to learn, how complicated the model is, and so on. \r\n",
        "- **num_iters**: It is used to specify the number of combinations that are randomly tried. If num_iters is too less, finding the best combination is difficult, and if num_iters is too large, the processing time increases. It is important to find a balanced value for ‘n_iter’.\r\n",
        "  - **1 iteration**: One mini-batch forward & backward pass. That means a parameter (wights and biases) update. \r\n",
        "- **minibatch**:  Number of examples in 1 iteration\r\n",
        "- **epochs**: How many times we are running the dataset\r\n",
        "\r\n",
        "  - **1 epoch**: Running through the whole dataset once\r\n",
        "\r\n",
        "Before instantiation, we’ll initialize some hyperparameters like following."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZkoEr4OOA5K"
      },
      "source": [
        "# Hyperparameters\r\n",
        "\r\n",
        "batch_size = 45\r\n",
        "num_iters = 16010      #to get 10 epoch, per batch there is 1601 data\r\n",
        "input_dim = 28*28      # num_features = 784\r\n",
        "output_dim = 10\r\n",
        "\r\n",
        "learning_rate = 0.001\r\n",
        "# Device\r\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ES9AWVs81-rJ"
      },
      "source": [
        "Instantiating the model class and enabling GPU:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-NgN0ajODGj",
        "outputId": "c15413ec-e833-4604-a461-8aa1ec688cb6"
      },
      "source": [
        "'''\r\n",
        "INSTANTIATE MODEL CLASS\r\n",
        "'''\r\n",
        "model = LogisticRegressionModel(input_size=input_dim,\r\n",
        "                                num_classes=output_dim)\r\n",
        "# To enable GPU\r\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegressionModel(\n",
              "  (linear): Linear(in_features=784, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOq4vhoIsTPP"
      },
      "source": [
        "##Step#6: Construct loss and optimizer (select from PyTorch API)\r\n",
        "\r\n",
        "We need Cross Entropy Loss to calculate our loss before we backpropagate and update our parameters.\r\n",
        "\r\n",
        "`criterion = nn.CrossEntropyLoss() ` \r\n",
        "\r\n",
        "It does 2 things at the same time.\r\n",
        "\r\n",
        "1. Computes softmax **([Logistic or Sigmoid]/softmax function)**\r\n",
        "2. Computes Cross Entropy Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zu3RP73OHHR"
      },
      "source": [
        "# INSTANTIATE OPTIMIZER CLASS\r\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEgrNNRc4Sn5"
      },
      "source": [
        "##Step #7 : Training: forward, loss, backward, step\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "  \r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlRbcIvIAAxO"
      },
      "source": [
        "\r\n",
        "- Calling `.backward()` mutiple times accumulates the gradient (**by addition**) for each parameter. \r\n",
        "\r\n",
        "- This is why you should call `optimizer.zero_grad()` after each .step() call. \r\n",
        "\r\n",
        "- Note that following the first `.backward` call, a second call is only possible after you have performed another **forward pass**.\r\n",
        "\r\n",
        "- `optimizer.step` performs a parameter update based on the current gradient (**stored in .grad attribute of a parameter**)\r\n",
        "\r\n",
        "Simplified equation:\r\n",
        "\r\n",
        "- `parameters = parameters - learning_rate * parameters_gradients`\r\n",
        "- parameters $W$ and $b$ in ($y = W^T * x + b$)\r\n",
        "- $\\theta = \\theta - \\eta \\cdot \\nabla_\\theta$  [ General parameter $\\theta$ ]\r\n",
        "  *  $\\theta$ : parameters (our variables)\r\n",
        "  *  $\\eta$ : learning rate (how fast we want to learn)\r\n",
        "  *  $\\nabla_\\theta$ : parameters' gradients"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6awpyoCAzjA"
      },
      "source": [
        "This traing is done with tuning the hyperparameters. We have done the training in 3 settings.\r\n",
        "\r\n",
        "###Setting 1: Accuracy32%\r\n",
        "**batch =45,itr = 16010, lr=0.05**\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6Yn9UC1OJJ8"
      },
      "source": [
        "'''\r\n",
        "TRAIN THE MODEL\r\n",
        "'''\r\n",
        "num_epochs = num_iters / (len(train_dataset) / batch_size)\r\n",
        "num_epochs = int(num_epochs)\r\n",
        "print(num_epochs)\r\n",
        "iteration_loss = []\r\n",
        "iter = 0\r\n",
        "for epoch in range(num_epochs):\r\n",
        "    for i, (images, labels) in enumerate(train_loader):\r\n",
        "\r\n",
        "        images = images.view(-1, 28*28).to(device)\r\n",
        "        labels = labels.to(device)\r\n",
        "\r\n",
        "        # Clear gradients w.r.t. parameters\r\n",
        "        optimizer.zero_grad()\r\n",
        "\r\n",
        "        # Forward pass to get output/logits\r\n",
        "        logits, probas = model(images) \r\n",
        "\r\n",
        "        # Calculate Loss: PyTorch implementation of CrossEntropyLoss works with logits, not probabilities\r\n",
        "        loss = F.cross_entropy(logits, labels)\r\n",
        "\r\n",
        "        # Getting gradients w.r.t. parameters\r\n",
        "        loss.backward()\r\n",
        "\r\n",
        "        # Updating parameters\r\n",
        "        optimizer.step()\r\n",
        "\r\n",
        "        iter += 1\r\n",
        "\r\n",
        "        if iter % 500 == 0:\r\n",
        "            # Calculate Accuracy         \r\n",
        "            correct = 0\r\n",
        "            total = 0\r\n",
        "            # Iterate through test dataset\r\n",
        "            for images, labels in test_loader:\r\n",
        "               \r\n",
        "                images = images.view(-1, 28*28).to(device)\r\n",
        "\r\n",
        "                # Forward pass only to get logits/output\r\n",
        "                logits, probas = model(images)\r\n",
        "\r\n",
        "                # Get predictions from the maximum value\r\n",
        "                _, predicted = torch.max(probas, 1)\r\n",
        "\r\n",
        "                # Total number of labels\r\n",
        "                total += labels.size(0)\r\n",
        "\r\n",
        "\r\n",
        "                # Total correct predictions\r\n",
        "                if torch.cuda.is_available():\r\n",
        "                    correct += (predicted.cpu() == labels.cpu()).sum() \r\n",
        "                else:\r\n",
        "                    correct += (predicted == labels).sum()\r\n",
        "\r\n",
        "            accuracy = 100 * correct.item() / total\r\n",
        "\r\n",
        "            # Print Loss\r\n",
        "            iteration_loss.append(loss.item())\r\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESecyXUHhCa8"
      },
      "source": [
        "print(len(train_dataset))\r\n",
        "print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AP2WLzE_5vh"
      },
      "source": [
        "#### Plot of predicted and actual values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "id": "RTpkU-xoOLTh",
        "outputId": "ff9484d6-5f23-4b7a-d216-055b8157dd37"
      },
      "source": [
        "import matplotlib\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "print (iteration_loss)\r\n",
        "plt.plot(iteration_loss)\r\n",
        "plt.ylabel('Cross Entropy Loss')\r\n",
        "plt.xlabel('Iteration (in every 500)')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-5a31dbf88577>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0miteration_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cross Entropy Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'iteration_loss' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25nTTp0aOSCk",
        "outputId": "6e3a34b2-3fd2-45ef-c8e8-2f3760dd7d4f"
      },
      "source": [
        "from google.colab import drive\r\n",
        "\r\n",
        "drive.mount('/content/gdrive')\r\n",
        "\r\n",
        "root_path = '/content/gdrive/My Drive/Soft Computing/'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxuRKpw3BPyz"
      },
      "source": [
        "#### Saving Model to Directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dx1MMojhOUQ7"
      },
      "source": [
        "import pickle\r\n",
        "save_model = True\r\n",
        "\r\n",
        "\r\n",
        "if save_model is True:\r\n",
        "    # Saves only parameters\r\n",
        "    # wights & biases\r\n",
        "    torch.save(model.state_dict(), root_path + 'Problem1_setting2.pkl') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOGL1sLQBRzx"
      },
      "source": [
        "#### Loading Model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SapQWIW8OWbA"
      },
      "source": [
        "load_model = True\r\n",
        "\r\n",
        "if load_model is True:\r\n",
        "    model.load_state_dict(torch.load(root_path + 'Problem1_setting2.pkl'))\r\n",
        "    print('Trained Model Loaded')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8SDb9CKOYa8"
      },
      "source": [
        "for images, labels in test_loader:\r\n",
        "    break\r\n",
        "    \r\n",
        "fig, ax = plt.subplots(1, 5)\r\n",
        "for i in range(5):\r\n",
        "    ax[i].imshow(images[i].view(28, 28), cmap=matplotlib.cm.binary)\r\n",
        "\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXGaHqyHOam5"
      },
      "source": [
        "_, predictions = model.forward(images[:5].view(-1, 28*28).to(device))\r\n",
        "predictions = torch.argmax(predictions, dim=1)\r\n",
        "print('Predicted labels', predictions.cpu().numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBlisaQ-RlI5"
      },
      "source": [
        "Accuracy: 32%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1oBOAn5P8bX"
      },
      "source": [
        "###Setting2: Accuracy 35%\r\n",
        "**batch =32,itr = 26000, lr=0.01**\r\n",
        "\r\n",
        "To train the model again with this setting, we need to go bach to step 3 and 4. Then in Step 5 will change the setting with the follwing:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3Jw8iNBiqdO"
      },
      "source": [
        "# Hyperparameters\r\n",
        "\r\n",
        "batch_size = 32\r\n",
        "num_iters = 26000\r\n",
        "input_dim = 28*28 # num_features = 784\r\n",
        "output_dim = 10\r\n",
        "\r\n",
        "learning_rate = 0.01\r\n",
        "# Device\r\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7Hv4L8cG93i"
      },
      "source": [
        "Now, we have to go to step 6 and then train the model like follwing:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EzGcybaizOz"
      },
      "source": [
        "num_epochs = num_iters / (len(train_dataset) / batch_size)\r\n",
        "num_epochs = int(num_epochs)\r\n",
        "print(num_epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7WjDEO4iw3e"
      },
      "source": [
        "'''\r\n",
        "TRAIN THE MODEL\r\n",
        "'''\r\n",
        "\r\n",
        "iteration_loss = []\r\n",
        "iter = 0\r\n",
        "for epoch in range(num_epochs):\r\n",
        "    for i, (images, labels) in enumerate(train_loader):\r\n",
        "\r\n",
        "        images = images.view(-1, 28*28).to(device)\r\n",
        "        labels = labels.to(device)\r\n",
        "\r\n",
        "        # Clear gradients w.r.t. parameters\r\n",
        "        optimizer.zero_grad()\r\n",
        "\r\n",
        "        # Forward pass to get output/logits\r\n",
        "        logits, probas = model(images) \r\n",
        "\r\n",
        "        # Calculate Loss: PyTorch implementation of CrossEntropyLoss works with logits, not probabilities\r\n",
        "        loss = F.cross_entropy(logits, labels)\r\n",
        "\r\n",
        "        # Getting gradients w.r.t. parameters\r\n",
        "        loss.backward()\r\n",
        "\r\n",
        "        # Updating parameters\r\n",
        "        optimizer.step()\r\n",
        "\r\n",
        "        iter += 1\r\n",
        "\r\n",
        "        if iter % 500 == 0:\r\n",
        "            # Calculate Accuracy         \r\n",
        "            correct = 0\r\n",
        "            total = 0\r\n",
        "            # Iterate through test dataset\r\n",
        "            for images, labels in test_loader:\r\n",
        "               \r\n",
        "                images = images.view(-1, 28*28).to(device)\r\n",
        "\r\n",
        "                # Forward pass only to get logits/output\r\n",
        "                logits, probas = model(images)\r\n",
        "\r\n",
        "                # Get predictions from the maximum value\r\n",
        "                _, predicted = torch.max(probas, 1)\r\n",
        "\r\n",
        "                # Total number of labels\r\n",
        "                total += labels.size(0)\r\n",
        "\r\n",
        "\r\n",
        "                # Total correct predictions\r\n",
        "                if torch.cuda.is_available():\r\n",
        "                    correct += (predicted.cpu() == labels.cpu()).sum() \r\n",
        "                else:\r\n",
        "                    correct += (predicted == labels).sum()\r\n",
        "\r\n",
        "            accuracy = 100 * correct.item() / total\r\n",
        "\r\n",
        "            # Print Loss\r\n",
        "            iteration_loss.append(loss.item())\r\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))\r\n",
        "print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xpoEHL_BsrT"
      },
      "source": [
        "#### Plot of predicted and actual values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysMIaJ--mur_"
      },
      "source": [
        "print (iteration_loss)\r\n",
        "plt.plot(iteration_loss)\r\n",
        "plt.ylabel('Cross Entropy Loss')\r\n",
        "plt.xlabel('Iteration (in every 500)')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBphxH3OBt8L"
      },
      "source": [
        "#### Saving Model to Directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGKcxLw9nKKJ"
      },
      "source": [
        "if save_model is True:\r\n",
        "    # Saves only parameters\r\n",
        "    # wights & biases\r\n",
        "    torch.save(model.state_dict(), root_path + 'Problem1_setting3.pkl') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paw5H2heBy8o"
      },
      "source": [
        "#### LoadingModel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYa7k83XnPl1"
      },
      "source": [
        "load_model = True\r\n",
        "\r\n",
        "if load_model is True:\r\n",
        "    model.load_state_dict(torch.load(root_path + 'Problem1_setting3.pkl'))\r\n",
        "    print('Trained Model Loaded')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-cbCQJEnYzI"
      },
      "source": [
        "for images, labels in test_loader:\r\n",
        "    break\r\n",
        "    \r\n",
        "fig, ax = plt.subplots(1, 5)\r\n",
        "for i in range(5):\r\n",
        "    ax[i].imshow(images[i].view(28, 28), cmap=matplotlib.cm.binary)\r\n",
        "\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljnPdB-Cnfln"
      },
      "source": [
        "_, predictions = model.forward(images[:5].view(-1, 28*28).to(device))\r\n",
        "predictions = torch.argmax(predictions, dim=1)\r\n",
        "print('Predicted labels', predictions.cpu().numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ehkp98RIniWR"
      },
      "source": [
        "Accuracy: 35%\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPUm6hAAzxp4"
      },
      "source": [
        "###Setting 3: Accuracy 29%\r\n",
        "\r\n",
        "**batch =64,itr = 12000, lr=0.05**\r\n",
        "\r\n",
        "To train the model againn with this setting, we need to go bach to step 3 and 4. Then in Step 5 will change the setting with the follwing:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWmu1SPJ2SyV"
      },
      "source": [
        "# Hyperparameters\r\n",
        "\r\n",
        "batch_size = 64\r\n",
        "num_iters = 12000\r\n",
        "input_dim = 28*28 # num_features = 784\r\n",
        "output_dim = 10\r\n",
        "\r\n",
        "learning_rate = 0.05\r\n",
        "# Device\r\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlmbG2Dvkvtm"
      },
      "source": [
        "Now, we have to go to step 6 and then train the model like following:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXuSg3d65AwZ"
      },
      "source": [
        "num_epochs = num_iters / (len(train_dataset) / batch_size)\r\n",
        "num_epochs = int(num_epochs)\r\n",
        "print(num_epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejhb8EGz2emB"
      },
      "source": [
        "'''\r\n",
        "TRAIN THE MODEL\r\n",
        "'''\r\n",
        "\r\n",
        "iteration_loss = []\r\n",
        "iter = 0\r\n",
        "for epoch in range(num_epochs):\r\n",
        "    for i, (images, labels) in enumerate(train_loader):\r\n",
        "\r\n",
        "        images = images.view(-1, 28*28).to(device)\r\n",
        "        labels = labels.to(device)\r\n",
        "\r\n",
        "        # Clear gradients w.r.t. parameters\r\n",
        "        optimizer.zero_grad()\r\n",
        "\r\n",
        "        # Forward pass to get output/logits\r\n",
        "        logits, probas = model(images) \r\n",
        "\r\n",
        "        # Calculate Loss: PyTorch implementation of CrossEntropyLoss works with logits, not probabilities\r\n",
        "        loss = F.cross_entropy(logits, labels)\r\n",
        "\r\n",
        "        # Getting gradients w.r.t. parameters\r\n",
        "        loss.backward()\r\n",
        "\r\n",
        "        # Updating parameters\r\n",
        "        optimizer.step()\r\n",
        "\r\n",
        "        iter += 1\r\n",
        "\r\n",
        "        if iter % 500 == 0:\r\n",
        "            # Calculate Accuracy         \r\n",
        "            correct = 0\r\n",
        "            total = 0\r\n",
        "            # Iterate through test dataset\r\n",
        "            for images, labels in test_loader:\r\n",
        "               \r\n",
        "                images = images.view(-1, 28*28).to(device)\r\n",
        "\r\n",
        "                # Forward pass only to get logits/output\r\n",
        "                logits, probas = model(images)\r\n",
        "\r\n",
        "                # Get predictions from the maximum value\r\n",
        "                _, predicted = torch.max(probas, 1)\r\n",
        "\r\n",
        "                # Total number of labels\r\n",
        "                total += labels.size(0)\r\n",
        "\r\n",
        "\r\n",
        "                # Total correct predictions\r\n",
        "                if torch.cuda.is_available():\r\n",
        "                    correct += (predicted.cpu() == labels.cpu()).sum() \r\n",
        "                else:\r\n",
        "                    correct += (predicted == labels).sum()\r\n",
        "\r\n",
        "            accuracy = 100 * correct.item() / total\r\n",
        "\r\n",
        "            # Print Loss\r\n",
        "            iteration_loss.append(loss.item())\r\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))\r\n",
        "print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXNfPEwQA8Pb"
      },
      "source": [
        "####Plot of predicted and actual values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZ4c8XngBPrB"
      },
      "source": [
        "print (iteration_loss)\r\n",
        "plt.plot(iteration_loss)\r\n",
        "plt.ylabel('Cross Entropy Loss')\r\n",
        "plt.xlabel('Iteration (in every 500)')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1yNjodGlaFH"
      },
      "source": [
        "####Saving Model to Directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sW3JwFX6BWxV"
      },
      "source": [
        "import pickle\r\n",
        "save_model = True\r\n",
        "\r\n",
        "\r\n",
        "if save_model is True:\r\n",
        "    # Saves only parameters\r\n",
        "    # wights & biases\r\n",
        "    torch.save(model.state_dict(), root_path + 'numta_logistic_setting8.pkl') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJBMQBiOlb1k"
      },
      "source": [
        "####Loading Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W11uxg-CBXI2"
      },
      "source": [
        "load_model = True\r\n",
        "\r\n",
        "if load_model is True:\r\n",
        "    model.load_state_dict(torch.load(root_path + 'numta_logistic_setting8.pkl'))\r\n",
        "    print('Trained Model Loaded')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4O8frKGBXLz"
      },
      "source": [
        "for images, labels in test_loader:\r\n",
        "    break\r\n",
        "    \r\n",
        "fig, ax = plt.subplots(1, 5)\r\n",
        "for i in range(5):\r\n",
        "    ax[i].imshow(images[i].view(28, 28), cmap=matplotlib.cm.binary)\r\n",
        "\r\n",
        "plt.show()\r\n",
        "\r\n",
        "_, predictions = model.forward(images[:5].view(-1, 28*28).to(device))\r\n",
        "predictions = torch.argmax(predictions, dim=1)\r\n",
        "print('Predicted labels', predictions.cpu().numpy())\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1Sk_67UnY53"
      },
      "source": [
        "###Setting4: Accuracy 42%\r\n",
        "\r\n",
        "**batch =32,itr = 20000, lr=0.01**\r\n",
        "\r\n",
        "To train the model againn with this setting, we need to go bach to step 3 and 4. Then in Step 5 will change the setting with the follwing:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w328YLl2ndpg"
      },
      "source": [
        "# Hyperparameters\r\n",
        "\r\n",
        "batch_size = 32\r\n",
        "num_iters = 20000\r\n",
        "input_dim = 28*28 # num_features = 784\r\n",
        "output_dim = 10\r\n",
        "\r\n",
        "learning_rate = 0.01\r\n",
        "# Device\r\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpcUaTIInfm_"
      },
      "source": [
        "Now, we have to go to step 6 and then train the model like follwing:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xG0i6JlmnnAr"
      },
      "source": [
        "num_epochs = num_iters / (len(train_dataset) / batch_size)\r\n",
        "num_epochs = int(num_epochs)\r\n",
        "print(num_epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8aHY2jGnpwi"
      },
      "source": [
        "'''\r\n",
        "TRAIN THE MODEL\r\n",
        "'''\r\n",
        "\r\n",
        "iteration_loss = []\r\n",
        "iter = 0\r\n",
        "for epoch in range(num_epochs):\r\n",
        "    for i, (images, labels) in enumerate(train_loader):\r\n",
        "\r\n",
        "        images = images.view(-1, 28*28).to(device)\r\n",
        "        labels = labels.to(device)\r\n",
        "\r\n",
        "        # Clear gradients w.r.t. parameters\r\n",
        "        optimizer.zero_grad()\r\n",
        "\r\n",
        "        # Forward pass to get output/logits\r\n",
        "        logits, probas = model(images) \r\n",
        "\r\n",
        "        # Calculate Loss: PyTorch implementation of CrossEntropyLoss works with logits, not probabilities\r\n",
        "        loss = F.cross_entropy(logits, labels)\r\n",
        "\r\n",
        "        # Getting gradients w.r.t. parameters\r\n",
        "        loss.backward()\r\n",
        "\r\n",
        "        # Updating parameters\r\n",
        "        optimizer.step()\r\n",
        "\r\n",
        "        iter += 1\r\n",
        "\r\n",
        "        if iter % 500 == 0:\r\n",
        "            # Calculate Accuracy         \r\n",
        "            correct = 0\r\n",
        "            total = 0\r\n",
        "            # Iterate through test dataset\r\n",
        "            for images, labels in test_loader:\r\n",
        "               \r\n",
        "                images = images.view(-1, 28*28).to(device)\r\n",
        "\r\n",
        "                # Forward pass only to get logits/output\r\n",
        "                logits, probas = model(images)\r\n",
        "\r\n",
        "                # Get predictions from the maximum value\r\n",
        "                _, predicted = torch.max(probas, 1)\r\n",
        "\r\n",
        "                # Total number of labels\r\n",
        "                total += labels.size(0)\r\n",
        "\r\n",
        "\r\n",
        "                # Total correct predictions\r\n",
        "                if torch.cuda.is_available():\r\n",
        "                    correct += (predicted.cpu() == labels.cpu()).sum() \r\n",
        "                else:\r\n",
        "                    correct += (predicted == labels).sum()\r\n",
        "\r\n",
        "            accuracy = 100 * correct.item() / total\r\n",
        "\r\n",
        "            # Print Loss\r\n",
        "            iteration_loss.append(loss.item())\r\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))\r\n",
        "print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xc8aiTErsnRZ"
      },
      "source": [
        "print(accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CzgAKmYnoTn"
      },
      "source": [
        "####Plot of predicted and actual values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItwBUA3ts3py"
      },
      "source": [
        "print (iteration_loss)\r\n",
        "plt.plot(iteration_loss)\r\n",
        "plt.ylabel('Cross Entropy Loss')\r\n",
        "plt.xlabel('Iteration (in every 500)')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWtt-ssHnvbi"
      },
      "source": [
        "####Saving Model to Directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVzfo_HltHNT"
      },
      "source": [
        "if save_model is True:\r\n",
        "    # Saves only parameters\r\n",
        "    # wights & biases\r\n",
        "    torch.save(model.state_dict(), root_path + 'Problem1_setting6.pkl') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUVJHlgEn0Bo"
      },
      "source": [
        "####Loading Model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlsYzQt3tfJE"
      },
      "source": [
        "load_model = True\r\n",
        "\r\n",
        "if load_model is True:\r\n",
        "    model.load_state_dict(torch.load(root_path + 'Problem1_setting6.pkl'))\r\n",
        "    print('Trained Model Loaded')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "niOewGJTtiwI"
      },
      "source": [
        "for images, labels in test_loader:\r\n",
        "    break\r\n",
        "    \r\n",
        "fig, ax = plt.subplots(1, 5)\r\n",
        "for i in range(5):\r\n",
        "    ax[i].imshow(images[i].view(28, 28), cmap=matplotlib.cm.binary)\r\n",
        "\r\n",
        "plt.show()\r\n",
        "\r\n",
        "_, predictions = model.forward(images[:5].view(-1, 28*28).to(device))\r\n",
        "predictions = torch.argmax(predictions, dim=1)\r\n",
        "print('Predicted labels', predictions.cpu().numpy())\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_y_8byMku05k"
      },
      "source": [
        "Accuracy:42%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7gyEzJq9Bt3"
      },
      "source": [
        "###Setting5 : Accuracy 37%\r\n",
        "\r\n",
        "**batch =64,itr = 12000, lr=0.005**\r\n",
        "\r\n",
        "To train the model againn with this setting, we need to go bach to step 3 and 4. Then in Step 5 will change the setting with the follwing:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSiAD4mbti8b"
      },
      "source": [
        "# Hyperparameters\r\n",
        "\r\n",
        "batch_size = 64\r\n",
        "num_iters = 12000\r\n",
        "input_dim = 28*28 # num_features = 784\r\n",
        "output_dim = 10\r\n",
        "\r\n",
        "learning_rate = 0.005\r\n",
        "# Device\r\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmDxjOcmnhkG"
      },
      "source": [
        "Now, we have to go to step 6 and then train the model like follwing:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1b8FujE7tjAI"
      },
      "source": [
        "num_epochs = num_iters / (len(train_dataset) / batch_size)\r\n",
        "num_epochs = int(num_epochs)\r\n",
        "print(num_epochs)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWftjNDr9L38"
      },
      "source": [
        "'''\r\n",
        "TRAIN THE MODEL\r\n",
        "'''\r\n",
        "iteration_loss = []\r\n",
        "iter = 0\r\n",
        "for epoch in range(num_epochs):\r\n",
        "    for i, (images, labels) in enumerate(train_loader):\r\n",
        "\r\n",
        "        images = images.view(-1, 28*28).to(device)\r\n",
        "        labels = labels.to(device)\r\n",
        "\r\n",
        "        # Clear gradients w.r.t. parameters\r\n",
        "        optimizer.zero_grad()\r\n",
        "\r\n",
        "        # Forward pass to get output/logits\r\n",
        "        logits, probas = model(images) \r\n",
        "\r\n",
        "        # Calculate Loss: PyTorch implementation of CrossEntropyLoss works with logits, not probabilities\r\n",
        "        loss = F.cross_entropy(logits, labels)\r\n",
        "\r\n",
        "        # Getting gradients w.r.t. parameters\r\n",
        "        loss.backward()\r\n",
        "\r\n",
        "        # Updating parameters\r\n",
        "        optimizer.step()\r\n",
        "\r\n",
        "        iter += 1\r\n",
        "\r\n",
        "        if iter % 500 == 0:\r\n",
        "            # Calculate Accuracy         \r\n",
        "            correct = 0\r\n",
        "            total = 0\r\n",
        "            # Iterate through test dataset\r\n",
        "            for images, labels in test_loader:\r\n",
        "               \r\n",
        "                images = images.view(-1, 28*28).to(device)\r\n",
        "\r\n",
        "                # Forward pass only to get logits/output\r\n",
        "                logits, probas = model(images)\r\n",
        "\r\n",
        "                # Get predictions from the maximum value\r\n",
        "                _, predicted = torch.max(probas, 1)\r\n",
        "\r\n",
        "                # Total number of labels\r\n",
        "                total += labels.size(0)\r\n",
        "\r\n",
        "\r\n",
        "                # Total correct predictions\r\n",
        "                if torch.cuda.is_available():\r\n",
        "                    correct += (predicted.cpu() == labels.cpu()).sum() \r\n",
        "                else:\r\n",
        "                    correct += (predicted == labels).sum()\r\n",
        "\r\n",
        "            accuracy = 100 * correct.item() / total\r\n",
        "\r\n",
        "            # Print Loss\r\n",
        "            iteration_loss.append(loss.item())\r\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TB9yiv9qn7Sj"
      },
      "source": [
        "####Plot of predicted and actual values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8Rtbl8S9L_F"
      },
      "source": [
        "print (iteration_loss)\r\n",
        "plt.plot(iteration_loss)\r\n",
        "plt.ylabel('Cross Entropy Loss')\r\n",
        "plt.xlabel('Iteration (in every 500)')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqJnSKIGoFCh"
      },
      "source": [
        "####Saving Model to directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbiJwdKU9MB3"
      },
      "source": [
        "import pickle\r\n",
        "save_model = True\r\n",
        "\r\n",
        "\r\n",
        "if save_model is True:\r\n",
        "    # Saves only parameters\r\n",
        "    # wights & biases\r\n",
        "    torch.save(model.state_dict(), root_path + 'Problem1_setting9.pkl') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eo9uPi4EoIS-"
      },
      "source": [
        "####Loading Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLK4Z1Iq9MG2"
      },
      "source": [
        "load_model = True\r\n",
        "\r\n",
        "if load_model is True:\r\n",
        "    model.load_state_dict(torch.load(root_path + 'Problem1_setting9.pkl'))\r\n",
        "    print('Trained Model Loaded')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJQurFYQ9MKP"
      },
      "source": [
        "for images, labels in test_loader:\r\n",
        "    break\r\n",
        "    \r\n",
        "fig, ax = plt.subplots(1, 5)\r\n",
        "for i in range(5):\r\n",
        "    ax[i].imshow(images[i].view(28, 28), cmap=matplotlib.cm.binary)\r\n",
        "\r\n",
        "plt.show()\r\n",
        "\r\n",
        "_, predictions = model.forward(images[:5].view(-1, 28*28).to(device))\r\n",
        "predictions = torch.argmax(predictions, dim=1)\r\n",
        "print('Predicted labels', predictions.cpu().numpy())\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sve9P-cE9Fp"
      },
      "source": [
        "Accuracy 37%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrlsixDYmcno"
      },
      "source": [
        "##Analyzing the Tuning of Hyperparameters:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vODAjngjmc_q"
      },
      "source": [
        "\r\n",
        "| Setting No.| Batch size | Dataset per batch | Iterations| Epochs| Learning rate    | Accuracy| Model (.pkl)|\r\n",
        "| :-------------: | :----------: | :-----------: | :-------------: | :----------: | :-----------: | :-----------: | :-----------: |\r\n",
        "| 01 | 45 | 1602 |  16010 | 10 | 0.001   | 32%   |Problem1_setting2.pkl|\r\n",
        "| 02 | 32 | 2253 |  26000 | 11 | 0.01    | 35%   |Problem1_setting3.pkl|\r\n",
        "| 03 | 64 | 1127 |  12000 | 10 | 0.05    | 29%   |Problem1_setting8.pkl|\r\n",
        "| 04 | 32 | 2253 |  20000 | 8  | 0.01    | **42%** |Problem1_setting6.pkl|\r\n",
        "| 05 | 64 | 1127 |  12000 | 10 | 0.005   | 37%   |Problem1_setting9.pkl|\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmsfe0semdf1"
      },
      "source": [
        "Here, we can see the best accuracy among 5 settings is 42%. We can analyze this taking each parameter in comparison.\r\n",
        "\r\n",
        "- **Analyzing according to learning rate:**\r\n",
        "\r\n",
        "\r\n",
        "<div align=\"center\">\r\n",
        "<img src=\"https://drive.google.com/uc?id=1yVSA7G6GamscBKufi7xIg7ttd7ue7vQu\" width=\"300\">\r\n",
        "</div>\r\n",
        "\r\n",
        "\r\n",
        "We can see the more learning rate increases, the more accuracy increases. But after some point, accuracy starts to decrease again.\r\n",
        "\r\n",
        "From this it is said that, A **learning rate** that is too **low** will take a long time to converge.Here, with 0.001 , accuracy is 32%. with more epochs and batch size, it may converge to a good result, but take too long time. If **learning rate** is too **large**, so SGD jumps too far and misses the area near local minima. This would be extreme case of \"under-fitting\".  Larning rate should be somewhere in between. For this situtation, it is at **0.01**.\r\n",
        "\r\n",
        "\r\n",
        "- **Analyzing according to epochs:**\r\n",
        "\r\n",
        "<div align=\"center\">\r\n",
        "<img src=\"https://drive.google.com/uc?id=1aR4eGYMlQYDgar-bjs91KCow21cg7XKq\" width=\"300\">\r\n",
        "</div>\r\n",
        "\r\n",
        "In general, the models improve with more epochs of training, to a point. They'll start to plateau in accuracy as they converge. But, here comparing setting 2 and 4, we can see as the epoch increases , accuracy decreases. So, we can say it is dataset dependent. After going to global minima, it starts to decrease again.\r\n",
        "\r\n",
        "\r\n",
        "- **Analyzing according to batch size and iterations:**\r\n",
        "\r\n",
        "\r\n",
        "<div align=\"center\">\r\n",
        "<img src=\"https://drive.google.com/uc?id=1q3EdUZJRpG49RdyWBINHeCOuAyd5Ejzx\" width=\"300\">\r\n",
        "</div>\r\n",
        "\r\n",
        "In general, larger batch sizes result in faster progress in training, but don't always converge as fast. Smaller batch sizes train slower, but can converge faster.  In setting 1 and 5, larger batch size with same epoch (by adjusting iterations) gives better acuracy.But in setting 1 and 3, larger batch size with same epoch (by adjusting iterations) gives lower accyracy. So, it's definitely problem dependent.\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "- **Analyzing the graphs of all the settings according to loss functions:**\r\n",
        "\r\n",
        "<div align=\"center\">\r\n",
        "<img src=\"https://drive.google.com/uc?id=1yBcSAPlt5TFtzvOy5EdF_a8z5PAWkZ1w\" width=\"800\">\r\n",
        "</div>\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "Nna3cpb32ZdK",
        "outputId": "64cfc14a-c9ae-4df1-cdc3-48c3b4243fc8"
      },
      "source": [
        "import matplotlib\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "X=[0.001 ,0.005,0.01,0.01,0.05 ]\r\n",
        "Y=[0.32,0.37,0.35,0.42,0.29]\r\n",
        "plt.plot(X,Y)\r\n",
        "plt.ylabel('Accuracy')\r\n",
        "plt.xlabel('Learning Rate')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "NLYQkrrE-fOU",
        "outputId": "5e75c0f6-8855-4b51-b00c-fccaea274af3"
      },
      "source": [
        "import matplotlib\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "X=[8,10,10,10,11 ]\r\n",
        "Y=[0.42,0.32,0.29,0.37,0.35]\r\n",
        "plt.plot(X,Y)\r\n",
        "plt.ylabel('Accuracy')\r\n",
        "plt.xlabel('Number of Epochs')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "-Dk89dm7Dwz7",
        "outputId": "42cdb1de-834f-4b40-a3c6-62230aea09f6"
      },
      "source": [
        "import matplotlib\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "X=[32,32,45,64,64 ]\r\n",
        "Y=[0.35,0.42,0.32,0.29,0.37]\r\n",
        "plt.plot(X,Y)\r\n",
        "plt.ylabel('Accuracy')\r\n",
        "plt.xlabel('Batch size')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}